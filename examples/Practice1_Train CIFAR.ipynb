{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "from PIL import Image  #注意Image,后面会用到\n",
    "\n",
    "import os \n",
    "\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(filename):\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data_path = \"datasets/cifar10/\"\n",
    "\n",
    "meta = unpickle(data_path + '/batches.meta')\n",
    "label_name = meta[b'label_names']\n",
    "\n",
    "for i in range(1,6):\n",
    "    content = unpickle(data_path + '/data_batch_' + str(i))\n",
    "    print('loading data...')\n",
    "    print(content.keys())\n",
    "    print('transforming data_batch' + str(i))\n",
    "    for j in range(10000):\n",
    "        img = content[b'data'][j]\n",
    "        img = img.reshape(3,32,32)\n",
    "        img = img.transpose(1,2,0)\n",
    "        \n",
    "        img_path = data_path + 'train/'+label_name[content[b'labels'][j]].decode()\n",
    "        if not os.path.isdir(img_path):\n",
    "            os.makedirs(img_path)\n",
    "            \n",
    "        img_name = img_path + '/batch_' + str(i) + '_num_' + str(j) +'.jpg'\n",
    "        \n",
    "        imsave(img_name,img)\n",
    "\n",
    "## test data\n",
    "content = unpickle(data_path + 'test_batch')\n",
    "print('loading data...')\n",
    "print(content.keys())\n",
    "print('transforming test_batch')\n",
    "for j in range(10000):\n",
    "    img = content[b'data'][j]\n",
    "    img = img.reshape(3,32,32)\n",
    "    img = img.transpose(1,2,0)\n",
    "        \n",
    "    img_path = data_path + 'test/'+label_name[content[b'labels'][j]].decode()\n",
    "    if not os.path.isdir(img_path):\n",
    "        os.makedirs(img_path)\n",
    "            \n",
    "    img_name = img_path + '/num_' +str(j) +'.jpg'\n",
    "        \n",
    "    imsave(img_name,img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords\n",
    "TensorFlow可以支持cifar10的数据格式， 也提供了标准的TFRecord 格式，而关于 tensorflow 读取数据， 官网提供了3中方法 \n",
    "1 Feeding： 在tensorflow程序运行的每一步， 用python代码在线提供数据 \n",
    "2 Reader ： 在一个计算图（tf.graph）的开始前，将文件读入到流（queue）中 \n",
    "3 在声明tf.variable变量或numpy数组时保存数据。受限于内存大小，适用于数据较小的情况\n",
    "\n",
    "在本文，主要介绍第二种方法，利用tf.record标准接口来读入文件,训练时不再使用feed,而是事先将数据的读取也构建成图，将数据读取图的输出当作训练输入直接与训练图拼接再一起，不需要再使用tf.palceholder,这样训练时tf将直接从硬盘中读取数据\n",
    "\n",
    "tfrecord, 这是一种将图像数据和标签放在一起的二进制文件，能更好的利用内存，在tensorflow中快速的复制，移动，读取，存储 等等..\n",
    "tf.train.Example 协议内存块包含了Features字段，通过feature将图片的二进制数据和label进行统一封装， 然后将example协议内存块转化为字符串， tf.python_io.TFRecordWriter 写入到TFRecords文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = \"datasets/cifar10/train/\"\n",
    "classes = ['airplane','automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] \n",
    "\n",
    "filename = data_path_train + \"cifar_train.tfrecords\"\n",
    "writer= tf.python_io.TFRecordWriter(filename) #要生成的文件\n",
    "\n",
    "for index in range(len(classes)):\n",
    "    class_path = data_path_train + classes[index] +'/'\n",
    "    \n",
    "    for img_name in os.listdir(class_path): \n",
    "        img_path=class_path+img_name #每一个图片的地址\n",
    "\n",
    "        img=Image.open(img_path)\n",
    "        #img= img.resize((128,128))\n",
    "        img_raw=img.tobytes()#将图片转化为二进制格式\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "            'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n",
    "        })) #example对象对label和image数据进行封装\n",
    "        writer.write(example.SerializeToString())  #序列化为字符串\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_test = \"datasets/cifar10/test/\"\n",
    "classes = ['airplane','automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] \n",
    "\n",
    "filename = data_path_test + \"cifar_test.tfrecords\"\n",
    "writer= tf.python_io.TFRecordWriter(filename) #要生成的文件\n",
    "\n",
    "for index in range(len(classes)):\n",
    "    class_path = data_path_test + classes[index] +'/'\n",
    "    \n",
    "    for img_name in os.listdir(class_path): \n",
    "        img_path=class_path+img_name #每一个图片的地址\n",
    "\n",
    "        img=Image.open(img_path)\n",
    "        #img= img.resize((128,128))\n",
    "        img_raw=img.tobytes()#将图片转化为二进制格式\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "            'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n",
    "        })) #example对象对label和image数据进行封装\n",
    "        writer.write(example.SerializeToString())  #序列化为字符串\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取TFRECORD文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename): # 读入dog_train.tfrecords\n",
    "    filename_queue = tf.train.string_input_producer([filename])#生成一个queue队列\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)#返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                       })#将image数据和label取出来\n",
    "\n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    img = tf.reshape(img, [32, 32, 3])  #reshape为32*32的3通道图片\n",
    "    img = tf.cast(img, tf.float32) * (1. / 255) - 0.5 #在流中抛出img张量\n",
    "    label = tf.cast(features['label'], tf.int32) #在流中抛出label张量\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 注意，feature的属性“label”和“img_raw”名称要和制作时统一\n",
    "## 返回的img数据和label数据一一对应。返回的img和label是2个 tf 张量\n",
    "'''\n",
    "(<tf.Tensor 'sub:0' shape=(32, 32, 3) dtype=float32>,\n",
    " <tf.Tensor 'Cast_1:0' shape=() dtype=int32>)\n",
    "'''\n",
    "\n",
    "read_and_decode(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示tfrecord格式的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = \"datasets/cifar10/train/\"\n",
    "\n",
    "filename_train = data_path_train + \"cifar_train.tfrecords\"\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([filename_train]) #读入流中\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)   #返回文件名和文件\n",
    "features = tf.parse_single_example(serialized_example,\n",
    "                                   features={\n",
    "                                       'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                       'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                   })  #取出包含image和label的feature对象\n",
    "\n",
    "image = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "image = tf.reshape(image, [32, 32, 3])\n",
    "label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "with tf.Session() as sess: #开始一个会话\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    ## \n",
    "    coord=tf.train.Coordinator()\n",
    "    threads= tf.train.start_queue_runners(coord=coord)\n",
    "    for i in range(10): ##  不使用epoch, 手动控制循环数量\n",
    "        example, l = sess.run([image,label])#在会话中取出image和label\n",
    "        img=Image.fromarray(example, 'RGB')#这里Image是之前提到的\n",
    "        img.save(data_path_train + str(i)+'_''Label_'+str(l)+'.jpg')#存下图片\n",
    "        #print(example, l)\n",
    "    ##     \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用TFrecod 数据训练\n",
    "（将以上代码清空输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size =100\n",
    "data_path_train = \"datasets/cifar10/train/\"\n",
    "\n",
    "filename_train = data_path_train + \"cifar_train.tfrecords\"\n",
    "\n",
    "filename_train_queue = tf.train.string_input_producer(\n",
    "    [filename_train], num_epochs=num_epochs) #读入流中, 并设定epoch数量\n",
    "\n",
    "reader_train = tf.TFRecordReader()\n",
    "\n",
    "_, serialized_example_train = reader_train.read(filename_train_queue)   #返回文件名和文件\n",
    "\n",
    "features_train = tf.parse_single_example(serialized_example_train,\n",
    "                                   features={\n",
    "                                       'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                       'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                   })  #取出包含image和label的feature对象\n",
    "\n",
    "image_train = tf.decode_raw(features_train['img_raw'], tf.uint8)\n",
    "image_train = tf.reshape(image_train, [32, 32, 3])\n",
    "image_train = tf.cast(image_train, tf.float32)*(1./255) - 0.5\n",
    "label_train = tf.cast(features_train['label'], tf.int32)\n",
    "\n",
    "images_batch_train, labels_batch_train = tf.train.shuffle_batch(\n",
    "    [image_train, label_train], batch_size=batch_size, \n",
    "    capacity=2000, \n",
    "    min_after_dequeue=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x): \n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]]) # number of filters\n",
    "    return tf.nn.relu(conv2d(input,W) + b)\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size,size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size,32,32,3])\n",
    "y = tf.placeholder(tf.float32, [batch_size,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "conv1_1 = conv_layer(x,shape=[3,3,3,32])\n",
    "conv1_2 = conv_layer(conv1_1,shape=[3,3,32,32])\n",
    "conv1_3 = conv_layer(conv1_2,shape=[3,3,32,32])\n",
    "conv1_pool = max_pool_2x2(conv1_3) # 16,32\n",
    "conv1_drop = tf.nn.dropout(conv1_pool,keep_prob=keep_prob)\n",
    "    \n",
    "conv2_1 = conv_layer(conv1_drop,shape=[3,3,32,64])\n",
    "conv2_2 = conv_layer(conv2_1,shape=[3,3,64,64])\n",
    "conv2_3 = conv_layer(conv2_2,shape=[3,3,64,64])\n",
    "conv2_pool = max_pool_2x2(conv2_3) # 8,64\n",
    "conv2_drop = tf.nn.dropout(conv2_pool,keep_prob=keep_prob)\n",
    "    \n",
    "conv3_1 = conv_layer(conv2_drop,shape=[3,3,64,128])\n",
    "conv3_2 = conv_layer(conv3_1,shape=[3,3,128,128])\n",
    "conv3_3 = conv_layer(conv3_2,shape=[3,3,128,128])\n",
    "conv3_pool = tf.nn.max_pool(conv3_3, ksize=(1,8,8,1), strides=(1,8,8,1), padding='SAME') # 1,128\n",
    "conv3_flat = tf.reshape(conv3_pool, [-1,128])\n",
    "conv3_drop = tf.nn.dropout(conv3_flat,keep_prob=keep_prob)\n",
    "    \n",
    "full1 = tf.nn.relu(full_layer(conv3_drop, 600))\n",
    "full1_drop = tf.nn.dropout(full1, keep_prob=keep_prob)\n",
    "    \n",
    "yout = full_layer(full1_drop,10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=yout,labels=y))\n",
    "\n",
    "learning_rate = 5e-4\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(yout,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-47eeb886e518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_batch_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    ##### coordinator ####\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    try:\n",
    "        step = 0\n",
    "        ## 事先已设定了epoch,不再需要控制循环次数\n",
    "        while not coord.should_stop():\n",
    "            step += 1\n",
    "            image_batch, label_batch = sess.run([images_batch_train, tf.one_hot(labels_batch_train,depth=10)])\n",
    "        \n",
    "            feed = {x:image_batch, y:label_batch, keep_prob:0.5}\n",
    "            # loss,_,acc=session.run([cross_entropy,train_step,accuracy], feed_dict=feed)\n",
    "            sess.run(train_step, feed_dict=feed, )\n",
    "        \n",
    "            if step%100 == 0:\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed)\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={x:image_batch, y:label_batch, keep_prob:1.0})\n",
    "                print(\"step {} loss: {}, train_acc: {:.4}%\".format(step, loss, train_accuracy*100))\n",
    "        \n",
    "            if step>1500:\n",
    "                learning_rate = learning_rate * 0.5\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training for %d epochs, %d steps.' % (num_epochs, step))\n",
    "    \n",
    "    finally:\n",
    "        # when done, ask the treads to stop.\n",
    "        coord.request_stop()\n",
    "    ## wait for threads to finish\n",
    "    coord.join(threads)\n",
    "  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size =100\n",
    "data_path = \"datasets/cifar10/test/\"\n",
    "\n",
    "filename = data_path + \"cifar_test.tfrecords\"\n",
    "\n",
    "filename_train_queue = tf.train.string_input_producer(\n",
    "    [filename_train], num_epochs=num_epochs) #读入流中, 并设定epoch数量\n",
    "\n",
    "reader_train = tf.TFRecordReader()\n",
    "\n",
    "_, serialized_example_train = reader_train.read(filename_train_queue)   #返回文件名和文件\n",
    "\n",
    "features_train = tf.parse_single_example(serialized_example_train,\n",
    "                                   features={\n",
    "                                       'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                       'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                   })  #取出包含image和label的feature对象\n",
    "\n",
    "image_train = tf.decode_raw(features_train['img_raw'], tf.uint8)\n",
    "image_train = tf.reshape(image_train, [32, 32, 3])\n",
    "image_train = tf.cast(image_train, tf.float32)*(1./255) - 0.5\n",
    "label_train = tf.cast(features_train['label'], tf.int32)\n",
    "\n",
    "images_batch_train, labels_batch_train = tf.train.shuffle_batch(\n",
    "    [image_train, label_train], batch_size=batch_size, \n",
    "    capacity=2000, \n",
    "    min_after_dequeue=1000)\n",
    "        \n",
    "        X = cifar.test.images.reshape(10,1000,32,32,3)\n",
    "        Y = cifar.test.labels.reshape(10,1000,10)\n",
    "        \n",
    "        test_accuracy = np.mean([sess.run(accuracy, feed_dict={x:X[i], y:Y[i], keep_prob:1.0}) for i in range(10)])\n",
    "        print(\"test_acc: {}\".format(test_accuracy))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
