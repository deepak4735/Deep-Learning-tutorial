[TOC]

# Early Stopping

[参考文献](<http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf>)

[原文地址](https://www.datalearner.com/blog/1051537860479157)

## 1. 简介

当我们训**练深度学习**神经网络的时候通常希望能获得最好的泛化性能（**generalization performance**，即可以很好地拟合数据）。但是所有的标准深度学习神经网络结构如全连接多层感知机都很容易**过拟合**：当网络在训练集上表现越来越好，错误率越来越低的时候，实际上在某一刻，它在测试集的表现已经开始变差。

模型的泛化能力通常使用模型在验证数据集（validation set）上的表现来评估。随着网络的优化，我们期望当模型在训练集上的误差降低的时候，其在验证集上的误差表现不会变差。反之，当模型在训练集上表现很好，在验证集上表现很差的时候，我们认为模型出现了过拟合（overfitting）的情况。

解决过拟合问题有两个方向：降低参数空间的维度或者降低每个维度上的有效规模（effective size）。

- 降低参数数量的方法包括greedy constructive learning、剪枝和权重共享等。
- 降低每个参数维度的有效规模的方法主要是正则化，如权重衰变（weight decay）和早停法（early stopping）等。



早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。其基本含义是在训练中计算模型在验证集上的表现，当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。其主要步骤如下：

1. 将原始的训练数据集划分成训练集和验证集
2. 只在训练集上进行训练，并每个一个周期计算模型在验证集上的误差，例如，每15次epoch（mini batch训练中的一个周期）
3. 当模型在验证集上的误差比上一次训练结果差的时候停止训练
4. 使用上一次迭代结果中的参数作为模型的最终参数

然而，在现实中，模型在验证集上的误差不会像上图那样平滑，而是像下图一样：

![img](https://www.researchgate.net/profile/Lutz_Prechelt/publication/2874749/figure/fig1/AS:645735506771969@1530966750067/A-real-validation-error-curve-Vertical-validation-set-error-horizontal-time-in.png)

<center> 图2、真实的验证集误差变化曲线</center>

也就是说，模型在验证集上的表现可能咱短暂的变差之后有可能继续变好。上图在训练集迭代到400次的时候出现了16个局部最低。其中有4个最低值是它们所在位置出现的时候的最低点。其中全局最优大约出现在第205次迭代中。首次出现最低点是第45次迭代。相比较第45次迭代停止，到第400次迭代停止的时候找出的最低误差比第45次提高了1.1%，但是训练时间大约是前者的7倍。

但是，并不是所有的误差曲线都像上图一样，有可能在出现第一次最低点之后，后面再也没有比当前最低点更低的情况了。所以我们看到，**早停法主要是训练时间和泛化错误之间的权衡。**尽管如此，也有某些停止标准也可以帮助我们寻找更好的权衡。



## 2. 使用

我们需要一个停止的标准来实施早停法，因此，我们希望它可以产生最低的泛化错误，同时也可以有最好的性价比，即给定泛化错误下的最小训练时间

### 2.1、停止标准

停止标准有很多，也很灵活，大约有三种。在给出早停法的具体标准之前，我们先确定一下符号。假设我们使用$E$作为训练算法的误差函数，那么$E_{tr}(t)$是训练数据上的误差，$E_{te}(t)$是测试集上的误差。实际情况下我们并不能知道泛化误差，因此我们使用验证集误差来估计它。

**第一类停止标准**

假设$E_{opt}(t)$是在迭代次数$t$时取得最好的验证集误差：
$$
E_{opt}(t) := \text{min}_{t'\leq t}E_{va}(t')
$$
我们定义一个新变量叫**泛化损失（generalization loss）**，它描述的是在当前迭代周期t中，泛化误差相比较目前的最低的误差的一个增长率：
$$
GL(t) = 100 \cdot \big( \frac{E_{va}(t)}{E_{opt}(t)} - 1 \big)
$$
较高的泛化损失显然是停止训练的一个候选标准，因为它直接表明了过拟合。这就是第一类的停止标准，即当泛化损失超过一定阈值的时候，停止训练。我们用$GL_{\alpha}$来定义，即当$GL_{\alpha}$大于一定值$\alpha$的时候，停止训练。

**第二类停止标准**

然而，当训练的速度很快的时候，我们可能希望模型继续训练。因为如果训练错误依然下降很快，那么泛化损失有很大概率被修复。我们通常会**假设过拟合只会在训练错误降低很慢的时候出现**。在这里，我们定义一个$k$周期，以及基于周期的一个新变量**度量进展（measure progress）**：
$$
P_k(t) = 1000 \cdot \big( \frac{ \sum_{t' = t-k+1}^t E_{tr}(t') }{ k \cdot min_{t' = t-k+1}^t E_{tr}(t') } -1 \big)
$$
它表达的含义是，当前的指定迭代周期内的平均训练错误比该期间最小的训练错误大多少。注意，当训练过程不稳定的时候，这个measure progress结果可能很大，其中训练错误会变大而不是变小。实际中，很多算法都由于选择了不适当的较大的步长而导致这样的抖动。除非全局都不稳定，否则在较长的训练之后，measure progress结果趋向于0（其实这个就是度量训练集错误在某段时间内的平均下降情况）。由此，我们引入了第二个停止标准，即泛化损失和进展的商$PQ_{\alpha}$大于指定值的时候停止，即$\frac{GL(t)}{P_k(t)} \gt \alpha$

**第三类停止标准**
第三类停止标准则完全依赖于泛化错误的变化，即当泛化错误在连续s个周期内增长的时候停止（UP）。

当验证集错误在连续s个周期内出现增长的时候，我们假设这样的现象表明了过拟合，它与错误增长了多大独立。这个停止标准可以度量局部的变化，因此可以用在剪枝算法中，即在训练阶段，允许误差可以比前面最小值高很多时候保留。

### 2.2、停止标准选择

一般情况下，“较慢”的标准会相对而言在平均水平上表现略好，可以提高泛化能力。然而，这些标准需要较长的训练时间。其实，总体而言，这些标准在系统性的区别很小。主要选择规则包括：

1. 除非较小的提升也有很大价值，负责选择较快的停止标准
2. 为了最大可能找到一个好的方案，使用GL标准
3. 为了最大化平均解决方案的质量，如果网络只是过拟合了一点点，可以使用PQ标准，否则使用UP标准

注意，目前并没有理论上可以证明那种停止标准较好，所以都是实验的数据。后续我们再介绍一下实验结果。



## 3. Keras 中的 early stopping

```python
# early stoppping
from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=2)

# 训练
history = model.fit(train_X, train_y, epochs=300, batch_size=20, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[early_stopping])
```

- monitor: 需要监视的量，val_loss，val_acc
- patience: 当early stop被激活(如发现loss相比上一个epoch训练没有下降)，则经过patience个epoch后停止训练
- verbose: 信息展示模式
- mode: 'auto','min','max'之一，
  - 在min模式训练，如果检测值停止下降则终止训练。
  - 在max模式下，当检测值不再上升的时候则停止训练。